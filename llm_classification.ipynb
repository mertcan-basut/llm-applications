{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7b+vWr/LFzJoqXObPxICo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mertcan-basut/llm-applications/blob/main/llm_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai\n",
        "!pip install -q tiktoken\n",
        "!pip install -q python-dotenv"
      ],
      "metadata": {
        "id": "K9Nct2MGK_Gw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"OPENAI_API_KEY=editme\" > .env"
      ],
      "metadata": {
        "id": "YdAGtaibgNpw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI as OpenAIClient\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv(), override=True) # read local .env file"
      ],
      "metadata": {
        "id": "yKmj9FwtLCST"
      },
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare dataset\n",
        "\n",
        "LLMs' contextual and semantic perception capabilities are exploited for classifying BBC news articles into 5 distinct categories: `tech`, `business`, `sport`, `entertainment`, and `politics`"
      ],
      "metadata": {
        "id": "qDjvrjVWYWiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kaggle API Token is downloaded from https://www.kaggle.com/settings\n",
        "# and uploaded to the file system's working directory\n",
        "\n",
        "!mkdir .kaggle\n",
        "!mv kaggle.json .kaggle/kaggle.json\n",
        "!chmod 600 .kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download yufengdev/bbc-fulltext-and-category\n",
        "!unzip bbc-fulltext-and-category.zip -d data/\n",
        "!rm bbc-fulltext-and-category.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pKrsUBoKIj-",
        "outputId": "4504de2a-4d01-4999-bc56-016a046077b6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/yufengdev/bbc-fulltext-and-category\n",
            "License(s): CC0-1.0\n",
            "Downloading bbc-fulltext-and-category.zip to /content\n",
            "  0% 0.00/1.83M [00:00<?, ?B/s]\n",
            "100% 1.83M/1.83M [00:00<00:00, 147MB/s]\n",
            "Archive:  bbc-fulltext-and-category.zip\n",
            "  inflating: data/bbc-text.csv       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_samples(data: pd.DataFrame, categories_col_name: str, samples_len: int):\n",
        "  \"\"\"\n",
        "  Get samples from the dataset while keeping the balance of the samples with respect to the classification categories.\n",
        "\n",
        "  Args:\n",
        "    data (pd.DataFrame): The dataset.\n",
        "    categories_col_name (str): The name of the column containing the classification categories.\n",
        "    samples_len (int): The number of samples to get.\n",
        "  \"\"\"\n",
        "  categories = data[categories_col_name].unique()\n",
        "  if samples_len < categories.size:\n",
        "    categories = np.random.choice(categories, size=samples_len, replace=False)\n",
        "    samples = pd.concat([data[data[categories_col_name] == category].sample(n=1, random_state=42) for category in categories])\n",
        "  else:\n",
        "    for _, test_index in StratifiedShuffleSplit(\n",
        "        n_splits=1, test_size=samples_len, random_state=42\n",
        "      ).split(data, data['category']): samples = data.iloc[test_index]\n",
        "  return samples"
      ],
      "metadata": {
        "id": "1V_n8hC6a8fm"
      },
      "execution_count": 289,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"data/bbc-text.csv\")\n",
        "data['category'] = data['category'].str.lower()\n",
        "categories = data['category'].unique()\n",
        "\n",
        "few_shot_samples = get_samples(data, 'category', 5)\n",
        "test_data = get_samples(data.drop(few_shot_samples.index), 'category', 100)"
      ],
      "metadata": {
        "id": "rTr2493mUqs_"
      },
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification using *function calling*"
      ],
      "metadata": {
        "id": "1PZQG1D8eKpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_descriptions = {\n",
        "  'tech': \"News articles or a piece of text about `technology` related topics.\",\n",
        "  'business': \"News articles or a piece of text about `business` related topics.\",\n",
        "  'sport': \"News articles or a piece of text about `sports` realted topics.\",\n",
        "  'entertainment': \"News articles or a piece of text about `entertainment` related topics.\",\n",
        "  'politics': \"News articles or a piece of text about `politics` related topics.\"\n",
        "}\n",
        "\n",
        "tools = [\n",
        "  {\n",
        "    'type': 'function',\n",
        "    'function': {\n",
        "      'name': key,\n",
        "      'description': value\n",
        "    }\n",
        "  } for key, value in category_descriptions.items()\n",
        "]\n",
        "\n",
        "sys_prompt = \"\"\"You are a news article classifier. \\\n",
        "Your task is to classify the category of the given article into \\\n",
        "tech, business, sport, entertainment, or politics.\n",
        "\n",
        "{few_shot_examples}\"\"\".format(\n",
        "  few_shot_examples=\"\\n\".join([f\"Text: ```{row['text']}```\\nCategory: {row['category']}\" for _, row in few_shot_samples.iterrows()])\n",
        ")\n",
        "\n",
        "usr_prompt = \"\"\"Text: ```{text}```\"\"\"\n",
        "\n",
        "client = OpenAIClient()\n",
        "llm_model_name = \"gpt-4-turbo\""
      ],
      "metadata": {
        "id": "9KRz8b8wh20w"
      },
      "execution_count": 340,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classification(text: str):\n",
        "  response = client.chat.completions.create(\n",
        "    model=llm_model_name,\n",
        "    messages=[\n",
        "      {'role': 'system', 'content': sys_prompt},\n",
        "      {'role': 'user', 'content': usr_prompt.format(text=text)}\n",
        "    ],\n",
        "    tools=tools,\n",
        "    tool_choice='required', # 'auto'|'required'\n",
        "    parallel_tool_calls=False,\n",
        "    temperature=0.0\n",
        "  )\n",
        "\n",
        "  category = response.choices[0].message.tool_calls[0].function.name\n",
        "  return category"
      ],
      "metadata": {
        "id": "l0MD-wvb2K6-"
      },
      "execution_count": 327,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['prediction'] = test_data['text'].apply(lambda text: classification(text))\n",
        "accuracy_score(test_data['category'], test_data['prediction'])"
      ],
      "metadata": {
        "id": "eeVE_JTZ88bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification using *log probabilities*"
      ],
      "metadata": {
        "id": "lRL1uZq1ePkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_descriptions = {\n",
        "  'tech': \"News articles about technology.\",\n",
        "  'business': \"News articles about business.\",\n",
        "  'sport': \"News articles about sports.\",\n",
        "  'entertainment': \"News articles about entertainment.\",\n",
        "  'politics': \"News articles about politics.\"\n",
        "}\n",
        "\n",
        "sys_prompt = \"\"\"You are a news article classifier. \\\n",
        "You are given a news article as text and you need to classify it into \\\n",
        "one of the following categories:\n",
        "\n",
        "{category_descriptions}\n",
        "\n",
        "{few_shot_examples}\"\"\".format(\n",
        "  category_descriptions=\"\\n\".join([f\"- `{key}`: {value}\" for key, value in category_descriptions.items()]),\n",
        "  few_shot_examples=\"\\n\".join([f\"Text: ```{row['text']}```\\nCategory: {row['category']}\" for _, row in few_shot_samples.iterrows()])\n",
        ")\n",
        "\n",
        "usr_prompt = \"\"\"Classify the text into tech, business, sport, entertainment, or politics. \\\n",
        "Text: ```{text}```\n",
        "Category: \"\"\"\n",
        "\n",
        "client = OpenAIClient()\n",
        "llm_model_name = \"gpt-4\"\n",
        "\n",
        "tokenizer = tiktoken.encoding_for_model(llm_model_name)\n",
        "categories_token = [token for category in categories for token in tokenizer.encode(category)] + [tokenizer.eot_token]\n",
        "max_tokens = max([len(tokenizer.encode(category)) for category in categories]) + 1"
      ],
      "metadata": {
        "id": "1CuqGRlczYFX"
      },
      "execution_count": 307,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classification(text: str):\n",
        "  response = client.chat.completions.create(\n",
        "    model=llm_model_name,\n",
        "    messages=[\n",
        "      {'role': 'system', 'content': sys_prompt},\n",
        "      {'role': 'user', 'content': usr_prompt.format(text=text)}\n",
        "    ],\n",
        "    logprobs=True,\n",
        "    logit_bias={token: 100 for token in categories_token},\n",
        "    max_tokens=max_tokens,\n",
        "    temperature=0.0\n",
        "  )\n",
        "\n",
        "  category = response.choices[0].message.content\n",
        "  confidence = np.array([np.exp(token.logprob) for token in response.choices[0].logprobs.content]).prod()\n",
        "  return category, confidence"
      ],
      "metadata": {
        "id": "g63iKJbZiWY9"
      },
      "execution_count": 311,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data['prediction'] = test_data['text'].apply(lambda text: classification(text)[0])\n",
        "accuracy_score(test_data['category'], test_data['prediction'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHsmt2ML22Ab",
        "outputId": "0d79f03e-b41e-4fe7-94a4-2a609d33581a"
      },
      "execution_count": 312,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.94"
            ]
          },
          "metadata": {},
          "execution_count": 312
        }
      ]
    }
  ]
}